---
title: 机器学习模型
date: 2024-07-09 14:18:54
categories: 机器学习
tags: 机器学习
cover: https://gitee.com/AsteroidQiao/library-management-system/raw/master/book-avatar/17205060501931720506049909.png
---
# 机器学习模型

机器学习是一种通过让计算机自动从数据中学习规律和模式，从而完成特定任务的方法。按照模型类型，机器学习可以分为两大类：**监督学习模型和无监督学习模型。**

附注：除了以上两大类模型，还有半监督学习和强化学习等其他类型的机器学习模型。半监督学习是指在有部分标签数据的情况下，结合监督学习和无监督学习的方法进行模型训练。强化学习是指通过让计算机自动与环境交互，学习出如何最大化奖励的策略。

不同的机器学习模型适用于不同的任务和场景。在实际应用中，需要根据具体的问题和数据特点选择合适的模型和方法。同时，机器学习也需要结合具体领域的知识和业务需求来进行深入研究和应用。

![image-20240709134536981](https://gitee.com/AsteroidQiao/library-management-system/raw/master/typora/2024-07-09/209585abf646ed28bddc14458a479060.png)

## 1. 有监督学习

有监督学习通常是利用带有*专家标注的标签的训练数据*，学习一个从输入变量X到输入变量Y的函数映射。Y = f (X)，训练数据通常是(n×x,y)的形式，其中n代表训练样本的大小，x和y分别是变量X和Y的样本值。

有监督学习可以被分为两类：

- 分类问题：预测某一样本所属的类别（离散的）。比如判断性别，是否健康等。
- 回归问题：预测某一样本的所对应的实数输出（连续的）。比如预测某一地区人的平均身高。

除此之外，集成学习也是一种有监督学习。它是将多个不同的相对较弱的机器学习模型的预测组合起来，用来预测新的样本。

### 1.1 单模型

#### 1.11 线性回归

- 模型简介：线性回归是一种用于建立自变量和因变量之间线性关系的统计模型。
- 实例：假设我们想要预测房屋的价格，我们可以使用房屋的面积、房间数量等特征作为自变量，房屋价格作为因变量。通过线性回归，我们可以找到一个线性方程，来描述自变量和因变量之间的关系。

线性回归是指完全由线性变量组成的回归模型。在线性回归分析中，只包括一个自变量和一个因变量，且二者的关系可用一条直线近似表示，这种回归分析称为`一元线性回归分析`。

如果回归分析中包括两个或两个以上的自变量，且自变量和因变量之间是线性关系，则称为`多元线性回归分析`。

![image-20240709134646542](https://gitee.com/AsteroidQiao/library-management-system/raw/master/typora/2024-07-09/b182498a3ca737e6929ff2f0e6da1254.png)

#### 1.12 逻辑回归

- 模型简介：逻辑回归是一种用于分类问题的统计模型，它将输入变量映射到一个概率值，表示属于某个类别的可能性。
- 实例：例如，我们想要预测一个人是否患有某种疾病，我们可以使用一些生理指标作为自变量，疾病状态作为因变量。通过逻辑回归，我们可以得到一个概率值，表示这个人患有疾病的可能性。

用于研究Y为定类数据时X和Y之间的影响关系情况，如果Y为两类比如0和1（比如1为愿意和0为不愿意，1为购买和0为不购买），此时就叫二元逻辑回归；如果Y为三类以上,此时就称为多分类逻辑回归。

自变量并不一定非要定类变量，它们也可以是定量变量。如果X是定类数据，此时需要对X进行哑变量设置。

![image-20240709134734430](https://gitee.com/AsteroidQiao/library-management-system/raw/master/typora/2024-07-09/0e7e87026275b5940366f8ddf97fab7a.png)

#### 1.13 决策树

- 模型简介：决策树是一种基于树结构的分类和回归模型，它通过对数据进行分割来构建决策规则。
- 实例：比如，我们想要根据天气情况来决定是否进行户外活动，我们可以使用天气特征（如温度、湿度、风速等）作为自变量，活动决策作为因变量。通过构建决策树，**我们可以根据不同的天气条件来做出决策**

决策树中每个内部节点都是一个分裂问题：指定了对实例的某个属性的测试，它将到达该节点的样本按照某个特定的属性进行分割，并且该节点的每一个后继分支对应于该属性的一个可能值。分类树叶节点所含样本中，其输出变量的众数就是分类结果。回归树的叶节点所含样本中，其输出变量的平均值就是预测结果。

![image-20240709135101895](https://gitee.com/AsteroidQiao/library-management-system/raw/master/typora/2024-07-09/4362d8b4c05b3ee3a2e7ff53324bb939.png)

#### 1.14 Lasso

![image-20240709135026539](https://gitee.com/AsteroidQiao/library-management-system/raw/master/typora/2024-07-09/eb7024b732c7138be59eeb50d6cc4da9.png)

Lasso方法是一种替代最小二乘法的压缩估计方法。Lasso的基本思想是建立一个L1正则化模型，在模型建立过程中会压缩一些系数和设定一些系数为零，当模型训练完成后，这些权值等于0的参数就可以舍去，从而使模型更为简单，并且有效防止模型过拟合。被广泛用于存在多重共线性数据的拟合和变量选择。

#### 1.15 K近邻(KNN)

KNN做回归和分类的主要区别在于最后做预测时候的决策方式不同。KNN做分类预测时，一般是选择多数表决法，即训练集里和预测的样本特征最近的K个样本，预测为里面有最多类别数的类别。KNN做回归时，一般是选择平均法，即最近的K个样本的样本输出的平均值作为回归预测值。但它们的理论是一样的。

#### 1.16 bp神经网络

bp神经网络是一种按误差逆传播算法训练的多层前馈网络，是目前应用最广泛的神经网络模型之一。bp神经网络的学习规则是使用最速下降法，通过反向传播来不断调整网络的权值和阈值，使网络的分类错误率最小（误差平方和最小）。

BP 神经网络是一种多层的前馈神经网络，其主要的特点是：信号是前向传播的，而误差是反向传播的。具体来说，对于如下的只含一个隐层的神经网络模型：

BP 神经网络的过程主要分为两个阶段，第一阶段是信号的前向传播，从输入层经过隐含层，最后到达输出层；第二阶段是误差的反向传播，从输出层到隐含层，最后到输入层，依次调节隐含层到输出层的权重和偏置，输入层到隐含层的权重和偏置。

![image-20240709135333010](https://gitee.com/AsteroidQiao/library-management-system/raw/master/typora/2024-07-09/6337a57d501ce623473fa4bdcfb21258.png)

#### 1.17 支持向量机(SVM)

支持向量机回归（SVR）用非线性映射将数据映射到高维数据特征空间中，使得在高维数据特征空间中自变量与因变量具有很好的线性回归特征，在该特征空间进行拟合后再返回到原始空间。

支持向量机分类（SVM）是一类按监督学习方式对数据进行二元分类的广义线性分类器，其决策边界是对学习样本求解的最大边距超平面。

![image-20240709135405077](https://gitee.com/AsteroidQiao/library-management-system/raw/master/typora/2024-07-09/cb67c04aa20c219d7d21fdabcd1ece66.png)

#### 1.18 朴素贝叶斯

在给定一个事件发生的前提下，计算另外一个事件发生的概率——我们将会使用贝叶斯定理。假设先验知识为d，为了计算我们的假设h为真的概率，我们将要使用如下贝叶斯定理：

![image-20240709135459028](https://gitee.com/AsteroidQiao/library-management-system/raw/master/typora/2024-07-09/7cedd1acd5ff47f3f456677d6236a954.png)

**该算法假定所有的变量都是相互独立的。**

![image-20240709135439493](https://gitee.com/AsteroidQiao/library-management-system/raw/master/typora/2024-07-09/a4e0f3174d4e26e1c5f36bd3ecce39e2.png)

### 1.2 集成学习

集成学习是一种将不同学习模型（比如分类器）的结果组合起来，通过投票或平均来进一步提高准确率。一般，对于分类问题用投票；对于回归问题用平均。这样的做法源于“众人拾材火焰高”的想法。

集成算法主要有三类：Bagging，Boosting 和Stacking。本文将不谈及stacking。

![image-20240709135600411](https://gitee.com/AsteroidQiao/library-management-system/raw/master/typora/2024-07-09/d65a634cc92e1cfb7444d3a3687af047.png)

#### Boosting

![image-20240709135627612](https://gitee.com/AsteroidQiao/library-management-system/raw/master/typora/2024-07-09/9b4bc6bd927a45a1f19ef2a17091adaf.png)

##### 1.21 GBDT

GBDT 是以 CART 回归树为基学习器的 Boosting 算法，是一个加法模型，它串行地训练一组 CART 回归树，最终对所有回归树的预测结果加和，由此得到一个强学习器，每一颗新树都拟合当前损失函数的负梯度方向。最后输出这一组回归树的加和，直接得到回归结果或者套用 sigmod 或者 softmax 函数获得二分类或者多分类结果。

##### 1.22 adaboost

adaboost给予误差率低的学习器一个高的权重，给予误差率高的学习器一个低的权重，结合弱学习器和对应的权重，生成强学习器。回归问题与分类问题算法的不同点在于误差率计算的方式不同，分类问题一般都采用0/1损失函数，而回归问题一般都是平方损失函数或者是线性损失函数。

##### 1.23 XGBoost

XGBoost 是"极端梯度上升"(Extreme Gradient Boosting)的简称，XGBoost 算法是一类由基函数与权重进行组合形成对数据拟合效果佳的合成算法。由于 XGBoost 模型具有较强的泛化能力、较高的拓展性、较快的运算速度等优势， 从2015年提出后便受到了统计学、数据挖掘、机器学习领域的欢迎。

xgboost是GBDT的一种高效实现，和GBDT不同，xgboost给损失函数增加了正则化项；且由于有些损失函数是难以计算导数的，xgboost使用损失函数的二阶泰勒展开作为损失函数的拟合。

##### 1.24 LightGBM

LightGBM 是 XGBoost 一种高效实现，其思想是将连续的浮点特征离散成 k 个离散值，并构造宽度为 k 的直方图。然后遍历训练数据，计算每个离散值在直方图中的累计统计量。在进行特征选择时，只需要根据直方图的离散值，遍历寻找最优的分割点；且使用带有深度限制的按叶子生长（leaf-wise）策略，节省了不少时间和空间上的开销。

##### 1.25 CatBoost

catboost 是一种基于对称决策树算法的 GBDT 框架，主要解决的痛点是高效合理地处理类别型特征和处理梯度偏差、预测偏移问题，提高算法的准确性和泛化能力。

#### Bagging

![image-20240709135834827](https://gitee.com/AsteroidQiao/library-management-system/raw/master/typora/2024-07-09/2082192ea9783c26b1faf36b82194d0b.png)

##### 1.26 随机森林

- 模型简介：随机森林是一种由多个决策树组成的集成学习模型，它通过随机选择特征和样本进行训练，提高了模型的泛化能力和准确性。
- 实例：例如，我们想要预测股票的涨跌，我们可以使用多个技术指标作为自变量，股票涨跌作为因变量。通过构建随机森林模型，我们可以综合多个决策树的预测结果，得到更准确的预测。

随机森林分类在生成众多决策树的过程中，是通过对建模数据集的样本观测和特征变量分别进行随机抽样，每次抽样结果均为一棵树，且每棵树都会生成符合自身属性的规则和分类结果(判断值)，而森林最终集成所有决策树的规则和分类结果(判断值)，实现随机森林算法的分类(回归)。

##### 1.27 Extra Trees

extra-trees (极其随机的森林)和随机森林非常类似，这里的“及其随机”表现在决策树的结点划分上，它干脆直接使用随机的特征和随机的阈值划分，这样我们每一棵决策树形状、差异就会更大、更随机。

## 2 无监督学习

无监督学习问题处理的是，只有输入变量X没有相应输出变量的训练数据。它利用没有专家标注训练数据，对数据的结构建模。

### 2.1 聚类

将相似的样本划分为一个簇（cluster）。与分类问题不同，聚类问题预先并不知道类别，自然训练数据也没有类别的标签。

#### 2.11 K-means算法

![image-20240709140050776](https://gitee.com/AsteroidQiao/library-management-system/raw/master/typora/2024-07-09/5555867f2da8e684fafdbe0cdf9c7572.png)

聚类分析是一种基于中心的聚类算法（K 均值聚类），通过迭代，将样本分到 K 个类中，使得每个样本与其所属类的中心或均值的距离之和最小。与分层聚类等按照字段进行聚类的算法不同的是，快速聚类分析是按照样本进行聚类。

#### 2.12 分层聚类

![image-20240709140109517](https://gitee.com/AsteroidQiao/library-management-system/raw/master/typora/2024-07-09/a0f965040e2eca3758b76c58d03de87a.png)

分层聚类法作为聚类的一种，是对给定数据对象的集合进行层次分解，根据分层分解采用的分解策略。层次聚类算法按数据分层建立簇，形成一棵以簇为节点的树。如果按自底向上进行层次分解，则称为凝聚的层次聚类，比如 AGNES。而按自顶向下的进行层次分解，则称为分裂法层次聚类，比如 DIANA。一般用的比较多的是凝聚层次聚类。

### 2.2 降维

降维指减少数据的维度同时保证不丢失有意义的信息。利用特征提取方法和特征选择方法，可以达到降维的效果。特征选择是指选择原始变量的子集。特征提取是将数据从高纬度转换到低纬度。广为熟知的主成分分析算法就是特征提取的方法。

#### 2.21 PCA主成分分析

![image-20240709140124812](https://gitee.com/AsteroidQiao/library-management-system/raw/master/typora/2024-07-09/aa037d98bac665455ec7419938ded158.png)

主成分分析将多个有一定相关性的指标进行线性组合，以最少的维度解释原数据中尽可能多的信息为目标进行降维，降维后的各变量间彼此线性无关，最终确定的新变量是原始变量的线性组合，且越往后主成分在方差中的比重也小，综合原信息的能力越弱。

#### 2.22 SVD奇异值分解

奇异值分解（SVD）是在机器学习领域广泛运用的算法，他不光可以用在降维算法中的特征值分解，还可以用于推荐系统，以及自然语言处理等领域，是很多算法的基石。

#### 2.23 LDA线性判别

![image-20240709140146131](https://gitee.com/AsteroidQiao/library-management-system/raw/master/typora/2024-07-09/66643f4ad506300778d5de75438a5ccd.png)

线性判别的原理是将样本投影到一条直线上，使得同类样本的投影点尽可能接近，不同样本的投影点尽可能远离；在对新样本进行分类时，将其投影到同样的直线上，再根据投影点的位置来确定新样本的类别。
