---
title: 特征工程
date: 2024-07-15 17:23:33
categories: 机器学习
tags: 机器学习
cover: https://gitee.com/AsteroidQiao/library-management-system/raw/master/typora/2024-07-15/2a8f5644f44ce167519e1b6d8513b41b.png
---
# 2、特征工程

## 2.1 数据集

- 目标
    - 知道数据集的分为训练集和测试集
    - 会使用sklearn的数据集
- 应用
    - 无

### 2.1.1 可用数据集

![img](https://gitee.com/AsteroidQiao/library-management-system/raw/master/typora/2024-07-15/2a8f5644f44ce167519e1b6d8513b41b.png)

Kaggle网址：https://www.kaggle.com/datasets

UCI数据集网址： http://archive.ics.uci.edu/ml/

scikit-learn网址：[http://scikit-learn.org/stable/datasets/index.html#datasets](http://scikit-learn.org/stable/datasets/index.html)

#### 1 Scikit-learn工具介绍

![scikitlearn](https://gitee.com/AsteroidQiao/library-management-system/raw/master/typora/2024-07-15/a3c59815644d4d8afc15648541e5c6d9.png)

- Python语言的机器学习工具
- Scikit-learn包括许多知名的机器学习算法的实现
- Scikit-learn文档完善，容易上手，丰富的API
- 目前稳定版本0.19.1

#### 2 安装

```
pip3 install Scikit-learn==0.19.1
```

安装好之后可以通过以下命令查看是否安装成功

```python
import sklearn
```

- 注：安装scikit-learn需要Numpy, Scipy等库

#### 3 Scikit-learn包含的内容

- 分类、聚类、回归
- 特征工程
- 模型选择、调优

### 2.1.2 sklearn数据集

#### 1 scikit-learn数据集API介绍

- sklearn.datasets
    - 加载获取流行数据集
    - datasets.load_*()
        - 获取小规模数据集，数据包含在datasets里
    - datasets.fetch_*(data_home=None)
        - 获取大规模数据集，需要从网络上下载，函数的第一个参数是data_home，表示数据集下载的目录,默认是 ~/scikit_learn_data/

#### 2 sklearn小数据集

- sklearn.datasets.load_iris()

  加载并返回鸢尾花数据集

![img](https://gitee.com/AsteroidQiao/library-management-system/raw/master/typora/2024-07-15/0ce21fc901b52f1abff50868e631c7c7.png)

- sklearn.datasets.load_boston()

  加载并返回波士顿房价数据集

![img](https://gitee.com/AsteroidQiao/library-management-system/raw/master/typora/2024-07-15/305394ea15e382dcdc24340341c349de.png)

#### 3 sklearn大数据集

- sklearn.datasets.fetch_20newsgroups(data_home=None,subset=‘train’)
    - subset：'train'或者'test'，'all'，可选，选择要加载的数据集。
    - 训练集的“训练”，测试集的“测试”，两者的“全部”

#### 4 sklearn数据集的使用

- 以鸢尾花数据集为例：

  ![img](https://gitee.com/AsteroidQiao/library-management-system/raw/master/typora/2024-07-15/4137bd5eafcfa795a6d9a214e932c661.png)

**sklearn数据集返回值介绍**

- load

  和fetch

  返回的数据类型datasets.base.Bunch(字典格式)

    - data：特征数据数组，是 [n_samples * n_features] 的二维 numpy.ndarray 数组
    - target：标签数组，是 n_samples 的一维 numpy.ndarray 数组
    - DESCR：数据描述
    - feature_names：特征名,新闻数据，手写数字、回归数据集没有
    - target_names：标签名

```python
from sklearn.datasets import load_iris
# 获取鸢尾花数据集
iris = load_iris()
print("鸢尾花数据集的返回值：\n", iris)
# 返回值是一个继承自字典的Bench
print("鸢尾花的特征值:\n", iris["data"])
print("鸢尾花的目标值：\n", iris.target)
print("鸢尾花特征的名字：\n", iris.feature_names)
print("鸢尾花目标值的名字：\n", iris.target_names)
print("鸢尾花的描述：\n", iris.DESCR)
```

**思考：拿到的数据是否全部都用来训练一个模型？**

### 2.1.3 数据集的划分

机器学习一般的数据集会划分为两个部分：

- 训练数据：用于训练，**构建模型**
- 测试数据：在模型检验时使用，用于**评估模型是否有效**

划分比例：

- 训练集：70% 80% 75%
- 测试集：30% 20% 25%

**数据集划分api**

- sklearn.model_selection.train_test_split(

  arrays, *

  options)

    - x 数据集的特征值
    - y 数据集的标签值
    - test_size 测试集的大小，一般为float
    - random_state 随机数种子,不同的种子会造成不同的随机采样结果。相同的种子采样结果相同。
    - return 测试集特征训练集特征值值，训练标签，测试标签(默认随机取)

```python
from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split


def datasets_demo():
    """
    对鸢尾花数据集的演示
    :return: None
    """
    # 1、获取鸢尾花数据集
    iris = load_iris()
    print("鸢尾花数据集的返回值：\n", iris)
    # 返回值是一个继承自字典的Bench
    print("鸢尾花的特征值:\n", iris["data"])
    print("鸢尾花的目标值：\n", iris.target)
    print("鸢尾花特征的名字：\n", iris.feature_names)
    print("鸢尾花目标值的名字：\n", iris.target_names)
    print("鸢尾花的描述：\n", iris.DESCR)

    # 2、对鸢尾花数据集进行分割
    # 训练集的特征值x_train 测试集的特征值x_test 训练集的目标值y_train 测试集的目标值y_test
    x_train, x_test, y_train, y_test = train_test_split(iris.data, iris.target, random_state=22)
    print("x_train:\n", x_train.shape)
    # 随机数种子
    x_train1, x_test1, y_train1, y_test1 = train_test_split(iris.data, iris.target, random_state=6)
    x_train2, x_test2, y_train2, y_test2 = train_test_split(iris.data, iris.target, random_state=6)
    print("如果随机数种子不一致：\n", x_train == x_train1)
    print("如果随机数种子一致：\n", x_train1 == x_train2)

    return None
```

## 2.2 特征工程介绍

学习目标

- 目标
    - 了解特征工程在机器学习当中的重要性
    - 知道特征工程的分类
- 应用
    - 无

![img](https://gitee.com/AsteroidQiao/library-management-system/raw/master/typora/2024-07-15/aa8334ee1662d975df84d3849926dfd4.png)

### 2.2.1 为什么需要特征工程(Feature Engineering)

> 机器学习领域的大神Andrew Ng(吴恩达)老师说“Coming up with features is difficult, time-consuming, requires expert knowledge. “Applied machine learning” is basically feature engineering. ”
>
> 注：业界广泛流传：数据和特征决定了机器学习的上限，而模型和算法只是逼近这个上限而已。

### 2.2.2 什么是特征工程

特征工程是使用**专业背景知识和技巧处理数据**，**使得特征能在机器学习算法上发挥更好的作用的过程**。

- 意义：会直接影响机器学习的效果

### 2.2.3 特征工程的位置与数据处理的比较

![特征工程过程](https://gitee.com/AsteroidQiao/library-management-system/raw/master/typora/2024-07-15/324e100ff44973dcc32279c028c04b56.png)

- pandas:一个数据读取非常方便以及基本的处理格式的工具
- sklearn:对于特征的处理提供了强大的接口

**特征工程包含内容**

- 特征抽取
- 特征预处理
- 特征降维

## 2.3 特征提取

学习目标

- 目标
    - 应用DictVectorizer实现对类别特征进行数值化、离散化
    - 应用CountVectorizer实现对文本特征进行数值化
    - 应用TfidfVectorizer实现对文本特征进行数值化
    - 说出两种文本特征提取的方式区别
- 应用
    - 无

什么是特征提取呢？

![文本特征提取图](https://gitee.com/AsteroidQiao/library-management-system/raw/master/typora/2024-07-15/8e34f475dddf46be972fac09e5147a9d.png)

![字典特征提取图](https://gitee.com/AsteroidQiao/library-management-system/raw/master/typora/2024-07-15/50e2db691bbe0936a491e6017fc32f0d.png)

### 2.3.1 特征提取

#### 1 将任意数据（如文本或图像）转换为可用于机器学习的数字特征

> 注：特征值化是为了计算机更好的去理解数据

- 字典特征提取(特征离散化)
- 文本特征提取
- 图像特征提取（深度学习将介绍）

#### 2 特征提取API

```python
sklearn.feature_extraction
```

### 2.3.2 字典特征提取

**作用：对字典数据进行特征值化**

- sklearn.feature_extraction.DictVectorizer(sparse=True,…)
    - DictVectorizer.fit_transform(X) X:字典或者包含字典的迭代器返回值：返回sparse矩阵
    - DictVectorizer.inverse_transform(X) X:array数组或者sparse矩阵 返回值:转换之前数据格式
    - DictVectorizer.get_feature_names() 返回类别名称

#### 1 应用

我们对以下数据进行特征提取

```python
[{'city': '北京','temperature':100}
{'city': '上海','temperature':60}
{'city': '深圳','temperature':30}]
```

![dictvec结果](https://gitee.com/AsteroidQiao/library-management-system/raw/master/typora/2024-07-15/1cdf4a6be2b9bf323ade4a00698019c3.png)

#### 2 流程分析

- 实例化类DictVectorizer
- 调用fit_transform方法输入数据并转换（注意返回格式）

```python
from sklearn.feature_extraction import DictVectorizer

def dict_demo():
    """
    对字典类型的数据进行特征抽取
    :return: None
    """
    data = [{'city': '北京','temperature':100}, {'city': '上海','temperature':60}, {'city': '深圳','temperature':30}]
    # 1、实例化一个转换器类
    transfer = DictVectorizer(sparse=False)
    # 2、调用fit_transform
    data = transfer.fit_transform(data)
    print("返回的结果:\n", data)
    # 打印特征名字
    print("特征名字：\n", transfer.get_feature_names())

    return None
```

注意观察没有加上sparse=False参数的结果

```python
返回的结果:
   (0, 1)    1.0
  (0, 3)    100.0
  (1, 0)    1.0
  (1, 3)    60.0
  (2, 2)    1.0
  (2, 3)    30.0
特征名字：
 ['city=上海', 'city=北京', 'city=深圳', 'temperature']
```

这个结果并不是我们想要看到的，所以加上参数，得到想要的结果：

```python
返回的结果:
 [[   0.    1.    0.  100.]
 [   1.    0.    0.   60.]
 [   0.    0.    1.   30.]]
特征名字：
 ['city=上海', 'city=北京', 'city=深圳', 'temperature']
```

之前在学习pandas中的离散化的时候，也实现了类似的效果。

我们把这个处理数据的技巧叫做”one-hot“编码：

![onehot](https://gitee.com/AsteroidQiao/library-management-system/raw/master/typora/2024-07-15/6f062fd692791124c3b9cbf70edfb6a3.png)

转化为：

![onehot1](https://gitee.com/AsteroidQiao/library-management-system/raw/master/typora/2024-07-15/8b4ac1c0dd7f2a8e3a41504555d61a8c.png)

#### 总结

**对于特征当中存在类别信息的我们都会做one-hot编码处理**

### 2.3.3 文本特征提取

**作用：对文本数据进行特征值化**

- **sklearn.feature_extraction.text.CountVectorizer(stop_words=[])**
    - 返回词频矩阵

- CountVectorizer.fit_transform(X) X:文本或者包含文本字符串的可迭代对象 返回值：返回sparse矩阵
- CountVectorizer.inverse_transform(X) X:array数组或者sparse矩阵 返回值:转换之前数据格
- CountVectorizer.get_feature_names() 返回值:单词列表

- **sklearn.feature_extraction.text.TfidfVectorizer**

#### 1 应用

我们对以下数据进行特征提取

```python
["life is short,i like python",
"life is too long,i dislike python"]
```

![countvec结果](https://gitee.com/AsteroidQiao/library-management-system/raw/master/typora/2024-07-15/783c4d4ce7f3e8d02f6c90730b825cae.png)

#### 2 流程分析

- 实例化类CountVectorizer
- 调用fit_transform方法输入数据并转换 （注意返回格式，利用toarray()进行sparse矩阵转换array数组）

```python
from sklearn.feature_extraction.text import CountVectorizer

def text_count_demo():
    """
    对文本进行特征抽取，countvetorizer
    :return: None
    """
    data = ["life is short,i like like python", "life is too long,i dislike python"]
    # 1、实例化一个转换器类
    # transfer = CountVectorizer(sparse=False)
    transfer = CountVectorizer()
    # 2、调用fit_transform
    data = transfer.fit_transform(data)
    print("文本特征抽取的结果：\n", data.toarray())
    print("返回特征名字：\n", transfer.get_feature_names())

    return None
```

返回结果：

```python
文本特征抽取的结果：
 [[0 1 1 2 0 1 1 0]
 [1 1 1 0 1 1 0 1]]
返回特征名字：
 ['dislike', 'is', 'life', 'like', 'long', 'python', 'short', 'too']
```

#### 问题:如果我们将数据替换成中文？

```python
"人生苦短，我喜欢Python" "生活太长久，我不喜欢Python"
```

那么最终得到的结果是

![不支持单个中文字](https://gitee.com/AsteroidQiao/library-management-system/raw/master/typora/2024-07-15/efdcf977c49a94f3f57d0fc7eb0893b4.png)

为什么会得到这样的结果呢，**仔细分析之后会发现英文默认是以空格分开的。其实就达到了一个分词的效果**，所以我们要对中文进行分词处理

#### 3 jieba分词处理

- jieba.cut()
    - 返回词语组成的生成器

需要安装下jieba库

```python
pip3 install jieba
```

#### 4 案例分析

对以下三句话进行特征值化

```python
今天很残酷，明天更残酷，后天很美好，
但绝对大部分是死在明天晚上，所以每个人不要放弃今天。

我们看到的从很远星系来的光是在几百万年之前发出的，
这样当我们看到宇宙时，我们是在看它的过去。

如果只用一种方式了解某样事物，你就不会真正了解它。
了解事物真正含义的秘密取决于如何将其与我们所了解的事物相联系。
```

- 分析
    - 准备句子，利用jieba.cut进行分词
    - 实例化CountVectorizer
    - 将分词结果变成字符串当作fit_transform的输入值

![三段中文结果](https://gitee.com/AsteroidQiao/library-management-system/raw/master/typora/2024-07-15/04a614e56649bc10648c6ed96925b1ae.png)

```python
from sklearn.feature_extraction.text import CountVectorizer
import jieba

def cut_word(text):
    """
    对中文进行分词
    "我爱北京天安门"————>"我 爱 北京 天安门"
    :param text:
    :return: text
    """
    # 用结巴对中文字符串进行分词
    text = " ".join(list(jieba.cut(text)))

    return text

def text_chinese_count_demo2():
    """
    对中文进行特征抽取
    :return: None
    """
    data = ["一种还是一种今天很残酷，明天更残酷，后天很美好，但绝对大部分是死在明天晚上，所以每个人不要放弃今天。",
            "我们看到的从很远星系来的光是在几百万年之前发出的，这样当我们看到宇宙时，我们是在看它的过去。",
            "如果只用一种方式了解某样事物，你就不会真正了解它。了解事物真正含义的秘密取决于如何将其与我们所了解的事物相联系。"]
    # 将原始数据转换成分好词的形式
    text_list = []
    for sent in data:
        text_list.append(cut_word(sent))
    print(text_list)

    # 1、实例化一个转换器类
    # transfer = CountVectorizer(sparse=False)
    transfer = CountVectorizer()
    # 2、调用fit_transform
    data = transfer.fit_transform(text_list)
    print("文本特征抽取的结果：\n", data.toarray())
    print("返回特征名字：\n", transfer.get_feature_names())

    return None
```

返回结果：

```python
Building prefix dict from the default dictionary ...
Dumping model to file cache /var/folders/mz/tzf2l3sx4rgg6qpglfb035_r0000gn/T/jieba.cache
Loading model cost 1.032 seconds.
['一种 还是 一种 今天 很 残酷 ， 明天 更 残酷 ， 后天 很 美好 ， 但 绝对 大部分 是 死 在 明天 晚上 ， 所以 每个 人 不要 放弃 今天 。', '我们 看到 的 从 很 远 星系 来 的 光是在 几百万年 之前 发出 的 ， 这样 当 我们 看到 宇宙 时 ， 我们 是 在 看 它 的 过去 。', '如果 只用 一种 方式 了解 某样 事物 ， 你 就 不会 真正 了解 它 。 了解 事物 真正 含义 的 秘密 取决于 如何 将 其 与 我们 所 了解 的 事物 相 联系 。']
Prefix dict has been built succesfully.
文本特征抽取的结果：
 [[2 0 1 0 0 0 2 0 0 0 0 0 1 0 1 0 0 0 0 1 1 0 2 0 1 0 2 1 0 0 0 1 1 0 0 1 0]
 [0 0 0 1 0 0 0 1 1 1 0 0 0 0 0 0 0 1 3 0 0 0 0 1 0 0 0 0 2 0 0 0 0 0 1 0 1]
 [1 1 0 0 4 3 0 0 0 0 1 1 0 1 0 1 1 0 1 0 0 1 0 0 0 1 0 0 0 2 1 0 0 1 0 0 0]]
返回特征名字：
 ['一种', '不会', '不要', '之前', '了解', '事物', '今天', '光是在', '几百万年', '发出', '取决于', '只用', '后天', '含义', '大部分', '如何', '如果', '宇宙', '我们', '所以', '放弃', '方式', '明天', '星系', '晚上', '某样', '残酷', '每个', '看到', '真正', '秘密', '绝对', '美好', '联系', '过去', '还是', '这样']
```

**但如果把这样的词语特征用于分类，会出现什么问题？**

请看问题：

![词语占比](https://gitee.com/AsteroidQiao/library-management-system/raw/master/typora/2024-07-15/710e1aa239e031b2a8b50245cb7e7406.png)

**该如何处理某个词或短语在多篇文章中出现的次数高这种情况**

#### 5 Tf-idf文本特征提取

- TF-IDF的主要思想是：如果**某个词或短语在一篇文章中出现的概率高，并且在其他文章中很少出现**，则认为此词或者短语具有很好的类别区分能力，适合用来分类。
- **TF-IDF作用：用以评估一字词对于一个文件集或一个语料库中的其中一份文件的重要程度。**

##### 5.1 公式

- 词频（term frequency，tf）指的是某一个给定的词语在该文件中出现的频率
- 逆向文档频率（inverse document frequency，idf）是一个词语普遍重要性的度量。某一特定词语的idf，可以**由总文件数目除以包含该词语之文件的数目，再将得到的商取以10为底的对数得到**

![tfidf公式](https://gitee.com/AsteroidQiao/library-management-system/raw/master/typora/2024-07-15/40ab9bef956d3724a16119a52de3acd6.png)

最终得出结果可以理解为重要程度。

```
注：假如一篇文件的总词语数是100个，而词语"非常"出现了5次，那么"非常"一词在该文件中的词频就是5/100=0.05。而计算文件频率（IDF）的方法是以文件集的文件总数，除以出现"非常"一词的文件数。所以，如果"非常"一词在1,000份文件出现过，而文件总数是10,000,000份的话，其逆向文件频率就是lg（10,000,000 / 1,0000）=3。最后"非常"对于这篇文档的tf-idf的分数为0.05 * 3=0.15
```

##### 5.2 案例

```python
from sklearn.feature_extraction.text import TfidfVectorizer
import jieba

def cut_word(text):
    """
    对中文进行分词
    "我爱北京天安门"————>"我 爱 北京 天安门"
    :param text:
    :return: text
    """
    # 用结巴对中文字符串进行分词
    text = " ".join(list(jieba.cut(text)))

    return text

def text_chinese_tfidf_demo():
    """
    对中文进行特征抽取
    :return: None
    """
    data = ["一种还是一种今天很残酷，明天更残酷，后天很美好，但绝对大部分是死在明天晚上，所以每个人不要放弃今天。",
            "我们看到的从很远星系来的光是在几百万年之前发出的，这样当我们看到宇宙时，我们是在看它的过去。",
            "如果只用一种方式了解某样事物，你就不会真正了解它。了解事物真正含义的秘密取决于如何将其与我们所了解的事物相联系。"]
    # 将原始数据转换成分好词的形式
    text_list = []
    for sent in data:
        text_list.append(cut_word(sent))
    print(text_list)

    # 1、实例化一个转换器类
    # transfer = CountVectorizer(sparse=False)
    transfer = TfidfVectorizer(stop_words=['一种', '不会', '不要'])
    # 2、调用fit_transform
    data = transfer.fit_transform(text_list)
    print("文本特征抽取的结果：\n", data.toarray())
    print("返回特征名字：\n", transfer.get_feature_names())

    return None
```

返回结果：

```python
Building prefix dict from the default dictionary ...
Loading model from cache /var/folders/mz/tzf2l3sx4rgg6qpglfb035_r0000gn/T/jieba.cache
Loading model cost 0.856 seconds.
Prefix dict has been built succesfully.
['一种 还是 一种 今天 很 残酷 ， 明天 更 残酷 ， 后天 很 美好 ， 但 绝对 大部分 是 死 在 明天 晚上 ， 所以 每个 人 不要 放弃 今天 。', '我们 看到 的 从 很 远 星系 来 的 光是在 几百万年 之前 发出 的 ， 这样 当 我们 看到 宇宙 时 ， 我们 是 在 看 它 的 过去 。', '如果 只用 一种 方式 了解 某样 事物 ， 你 就 不会 真正 了解 它 。 了解 事物 真正 含义 的 秘密 取决于 如何 将 其 与 我们 所 了解 的 事物 相 联系 。']
文本特征抽取的结果：
 [[ 0.          0.          0.          0.43643578  0.          0.          0.
   0.          0.          0.21821789  0.          0.21821789  0.          0.
   0.          0.          0.21821789  0.21821789  0.          0.43643578
   0.          0.21821789  0.          0.43643578  0.21821789  0.          0.
   0.          0.21821789  0.21821789  0.          0.          0.21821789
   0.        ]
 [ 0.2410822   0.          0.          0.          0.2410822   0.2410822
   0.2410822   0.          0.          0.          0.          0.          0.
   0.          0.2410822   0.55004769  0.          0.          0.          0.
   0.2410822   0.          0.          0.          0.          0.48216441
   0.          0.          0.          0.          0.          0.2410822
   0.          0.2410822 ]
 [ 0.          0.644003    0.48300225  0.          0.          0.          0.
   0.16100075  0.16100075  0.          0.16100075  0.          0.16100075
   0.16100075  0.          0.12244522  0.          0.          0.16100075
   0.          0.          0.          0.16100075  0.          0.          0.
   0.3220015   0.16100075  0.          0.          0.16100075  0.          0.
   0.        ]]
返回特征名字：
 ['之前', '了解', '事物', '今天', '光是在', '几百万年', '发出', '取决于', '只用', '后天', '含义', '大部分', '如何', '如果', '宇宙', '我们', '所以', '放弃', '方式', '明天', '星系', '晚上', '某样', '残酷', '每个', '看到', '真正', '秘密', '绝对', '美好', '联系', '过去', '还是', '这样']
```

#### 6 Tf-idf的重要性

**分类机器学习算法进行文章分类中前期数据处理方式**
